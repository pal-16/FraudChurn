# -*- coding: utf-8 -*-
"""k_means_anamoly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MFC9S-PkhK5v2rbJpnbzvUmRYTTm2r-Y

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy
import pandas as pd
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons
from sklearn.cluster import SpectralClustering

"""### Import Data & Preprocessing"""

url = 'https://drive.google.com/file/d/10WGB6yHEpyGJNKT_-JfR0_n2iurUnU2L/view?usp=sharing'
url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
account = pd.read_csv(url, sep = ';')

account.head()

"""#### Converting String Features to Catagorical Values"""

from sklearn.preprocessing import OneHotEncoder
frequency_encoder = OneHotEncoder()
account_encoder = frequency_encoder.fit_transform(account.frequency.values.reshape(-1,1)).toarray()

accountOneHot = pd.DataFrame(account_encoder, columns = ["frequency_"+str(int(i)) for i in range(account_encoder.shape[1])])
account = pd.concat([account, accountOneHot], axis=1)
account = account.drop('frequency', axis = 1)

account.head()

url = 'https://drive.google.com/file/d/1YJtFjOVIl1wV-UltDiZlvlWNJD9H7cVC/view?usp=sharing'
url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
order = pd.read_csv(url, sep = ';')

order.head()

"""#### Converting String Features to Catagorical Values"""

from sklearn.preprocessing import OneHotEncoder
bank_to_encoder = OneHotEncoder()
order_encoder = bank_to_encoder.fit_transform(order.bank_to.values.reshape(-1,1)).toarray()

orderOneHot1 = pd.DataFrame(order_encoder, columns = ["bank_to_"+str(int(i)) for i in range(order_encoder.shape[1])])
order = pd.concat([order, orderOneHot1, ], axis=1)
order = order.drop('bank_to', axis = 1)

order.head()

url = 'https://drive.google.com/file/d/1WTmM5qxJJLcDuHRKhW2v9jxb9UAcTj4Z/view?usp=sharing'
url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
transaction = pd.read_csv(url, sep = ';')

transaction.head()

"""#### Converting String Features to Catagorical Values"""

from sklearn.preprocessing import OneHotEncoder
type_encoder = OneHotEncoder()
operation_encoder = OneHotEncoder()
bank_encoder = OneHotEncoder()
transaction_encoder1 = type_encoder.fit_transform(transaction.type.values.reshape(-1,1)).toarray()
transaction_encoder2 = operation_encoder.fit_transform(transaction.operation.values.reshape(-1,1)).toarray()
transaction_encoder3 = bank_encoder.fit_transform(transaction.bank.values.reshape(-1,1)).toarray()

transactionOneHot1 = pd.DataFrame(transaction_encoder1, columns = ["type_"+str(int(i)) for i in range(transaction_encoder1.shape[1])])
transactionOneHot2 = pd.DataFrame(transaction_encoder2, columns = ["operation_"+str(int(i)) for i in range(transaction_encoder2.shape[1])])
transactionOneHot3 = pd.DataFrame(transaction_encoder3, columns = ["bank_"+str(int(i)) for i in range(transaction_encoder3.shape[1])])

transaction = pd.concat([transaction, transactionOneHot1, transactionOneHot2, transactionOneHot3], axis=1)
transaction = transaction.drop('type', axis = 1)
transaction = transaction.drop('operation', axis = 1)
transaction = transaction.drop('bank', axis = 1)

transaction.shape

"""### Dealing with Nan Data & Feature Selection

**Note:** After running a lot of experiments and observing behavior of PCA model we understood that their are a lot of Nan values for different columns which all can't be dealt in a similar way and we looked for total nan values and seeing if they could be dropped altogether.
"""

X = pd.concat([account, order, transaction], axis=0)

# dividing the data into train and test sets for the k-means model
X_new = X.copy() #create a copy of your data 
X_new.dropna(axis = 0, how='all', inplace=True)
X_new.fillna(0, inplace=True)

x_train = X_new.sample(frac=0.40, random_state=0)
x_test = X_new.drop(x_train.index)

X_new.head()

X_new.shape

X_new['order_id'].isna().sum()

X_new['account_id'].isna().sum()

X_new['trans_id'].isna().sum()

X_new['balance'].isna().sum()

"""> After figuring out which columns to drop, we created a subset of values which need to be dropped out, we filtered the dataset."""

df = x_train.drop(['k_symbol', 'order_id', 'trans_id'], axis = 1)

df.columns

"""#### PCA for Feature Selection"""

from sklearn.decomposition import PCA

pca = PCA(3) 
df_pca = pca.fit_transform(df)

pca.explained_variance_ratio_

plt.scatter(df_pca[:, 0], df_pca[:, 1])

plt.scatter(df_pca[:, 1], df_pca[:, 2])

plt.scatter(df_pca[:, 0], df_pca[:, 2])

print(type(df), df.shape)

kmeans = KMeans(n_clusters=2)
 
#predict the labels of clusters.
label = kmeans.fit_predict(df_pca)

u_labels = np.unique(label)
centroids = kmeans.cluster_centers_

#plotting the results:
for i in u_labels:
    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)
plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')
plt.legend()
plt.show()

"""### Finding "outliers" based on the distance from centers"""

# points array will be used to reach the index easy
points = np.empty((0,len(df_pca[0])), float)
# distances will be used to calculate outliers
distances = np.empty((0,len(df_pca[0])), float)

from scipy.spatial.distance import cdist
# getting points and distances
for i, center_elem in enumerate(centroids):
    # cdist is used to calculate the distance between center and other points
    distances = np.append(distances, cdist([center_elem],df_pca[label == i], 'euclidean')) 
    points = np.append(points, df_pca[label == i], axis=0)

percentile = 95
# getting outliers whose distances are greater than some percentile
outliers = points[np.where(distances > np.percentile(distances, percentile))]

df.shape

outliers.shape

def predict_outlier(test):
  return True

"""### Testing with a value"""

test_value = []
predict_outlier(test_value)

"""#### Isolation Method"""

!pip install pycaret==2.3.5
!pip install scipy==1.4.1

!pip uninstall numpy
!pip install numpy

!pip install --upgrade numpy

from pycaret.anomaly import setup, create_model, assign_model, plot_model

!numpy -v

anom = setup(data = df, silent = True)

anom_model = create_model(model = 'iforest', fraction = 0.05)
results = assign_model(anom_model)

plot_model(anom_model, plot = 'tsne')

plot_model(anom_model, plot = 'umap')